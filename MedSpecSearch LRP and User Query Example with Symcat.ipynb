{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\P\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\P\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Our code\n",
    "import lrp\n",
    "import EmbedHelper\n",
    "import DataLoader\n",
    "import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Fast Text', 2: 'Google News', 3: 'HealthTap', 4: 'Pubmed', 5: 'Glove', 6: 'iCliniq Trigram', 7: 'iCliniq default'}\n"
     ]
    }
   ],
   "source": [
    "embedDict = EmbedHelper.EmbeddingHandler.embedDict\n",
    "print(embedDict)\n",
    "configs = {\n",
    "    \"vectorSize\":300,\n",
    "    \"trainNewModel\":True,\n",
    "    \"dataColumn\":\"question\",\n",
    "    \"maxLength\":128,\n",
    "    \"batchSize\":8,\n",
    "    \"embeddingType\":embedDict[2],\n",
    "    \"ELMo\":True,\n",
    "    \"PreEmbed\":True,\n",
    "    \"restore\":True\n",
    "}\n",
    "\n",
    "inputSize = configs[\"maxLength\"]\n",
    "vectorSize = configs[\"vectorSize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Google News\n"
     ]
    }
   ],
   "source": [
    "EmbedModel = EmbedHelper.EmbeddingHandler(configs[\"embeddingType\"], False, 300, \"Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iCliniq Data (for training, testing model, and LRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data that is larger with 9800~ data instances\n",
    "trainData = np.load(\"data//icliniq//iCliniq_14K//icliniq_14k_train_questions.npy\")\n",
    "trainTarget = np.load(\"data//icliniq//iCliniq_14K//icliniq_14k_train_target.npy\")\n",
    "testData = np.load(\"data//icliniq//iCliniq_14K//icliniq_14k_test_questions.npy\")\n",
    "testTarget = np.load(\"data//icliniq//iCliniq_14K//icliniq_14k_test_target.npy\")\n",
    "\n",
    "trainData_raw = np.load(\"data//icliniq//iCliniq_14K//icliniq_14k_train_questions_raw.npy\")\n",
    "testData_raw = np.load(\"data//icliniq//iCliniq_14K//icliniq_14k_test_questions_raw.npy\")\n",
    "\n",
    "ClassDict = {}\n",
    "with open('fold0classDict.pkl', 'rb') as f:\n",
    "    ClassDict = pickle.load(f)\n",
    "outputSize = len(ClassDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symcat Data (used for asking keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = np.load(\"data//symcat//symcat_data.npy\")\n",
    "symptoms = np.load(\"data//symcat//symcat_categories.npy\")\n",
    "desc_plus_symptoms = np.load(\"data//symcat//symcat_data_plus_cats.npy\")\n",
    "softmax_results_symcat = np.load(\"data//symcat//softmax_results_symcat.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is only used if we want to get softmax output for symcat data\n",
    "# However, I already did that and saved as npy file\n",
    "\n",
    "# fake_labels = np.array([\"Dermatology\"] * len(symptoms))\n",
    "\n",
    "# raw_symcat_data = []\n",
    "# for sentence in desc_plus_symptoms:\n",
    "#     tmp = \"\"\n",
    "#     for word in sentence:\n",
    "#         tmp += word + \" \"\n",
    "    \n",
    "#     raw_symcat_data.append(tmp[0:-1])\n",
    "    \n",
    "# symcat_data_stacked = np.hstack((np.array(raw_symcat_data).reshape(-1,1), fake_labels.reshape(-1,1)))\n",
    "# testData,testTarget,_ = DataLoader.DataHandler.masterPreprocessor(symcat_data_stacked,shuffle=False,classDict=fold0ClassDict,maxLength=configs[\"maxLength\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokenLengths(token):\n",
    "    return [len(item) for item in token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePerformance(nnModel,sess,testData,testTarget,batchSize,uncertaintyCoef):\n",
    "    reverseClassDict = {value:key for key,value in ClassDict.items()}\n",
    "    top3 = []\n",
    "    \n",
    "    dataSize = testData.shape[0]\n",
    "    start = 0\n",
    "    end = batchSize\n",
    "    \n",
    "    totalAcc = 0\n",
    "    totalUcAcc = 0\n",
    "    totalDataRate = 0\n",
    "    \n",
    "    truth = None\n",
    "    predu = None\n",
    "    \n",
    "    testTruth = np.array([])\n",
    "    testPred = np.array([])\n",
    "    testScores = []\n",
    "    \n",
    "    testEvTrue = 0\n",
    "    testEvFail = 0\n",
    "    \n",
    "    while(start<dataSize):\n",
    "        data = np.array(testData[start:end])\n",
    "        dataClean = data\n",
    "        \n",
    "        if(configs[\"PreEmbed\"]):\n",
    "            data = EmbedModel.vectorizeBatch(data)\n",
    "        \n",
    "        outputData = np.array(testTarget[start:end])\n",
    "        cutSize = data.shape[0]\n",
    "        tokens_length = getTokenLengths(data)\n",
    "        \n",
    "        fd = {nnModel.nn_inputs:dataClean,nnModel.nn_vector_inputs:data,nnModel.nn_outputs:outputData,nnModel.isTraining:False,nnModel.token_lengths:tokens_length,\n",
    "             nnModel.uncertaintyRatio:uncertaintyCoef}\n",
    "        \n",
    "        scores, prob, testBAcc,nnTruth,nnPrediction,nnMatch,evCor,evFail,ucAcc,dataRate = sess.run([nnModel.scores, nnModel.prob, nnModel.accuracy,nnModel.truths,nnModel.predictions\n",
    "                                                                       ,nnModel.correct_predictions,nnModel.mean_ev_succ,nnModel.mean_ev_fail,nnModel.ucAccuracy,\n",
    "                                                                                     nnModel.dataRatio]\n",
    "                                                                      ,feed_dict=fd)\n",
    "        # For top 3\n",
    "        prob = prob[0]\n",
    "        probDict = {reverseClassDict[i]:prob[i] for i in np.arange(outputSize)}\n",
    "        probMatrix = []\n",
    "        for i in range(len(prob)):\n",
    "            probMatrix.append([reverseClassDict[i], prob[i]])\n",
    "        probMatrix = sorted(probMatrix, key=lambda x: (x[1]), reverse=True)\n",
    "        top3.append(probMatrix[0:3])\n",
    "        \n",
    "        testTruth = np.append(testTruth,nnTruth,axis=0)\n",
    "        testPred = np.append(testPred,nnPrediction,axis=0)\n",
    "#         testScores = np.append(testScores, scores, axis=0)\n",
    "        testScores.append(scores)\n",
    "        testEvTrue += evCor*cutSize\n",
    "        testEvFail += evFail*cutSize \n",
    "        \n",
    "        totalAcc += testBAcc*cutSize\n",
    "        totalUcAcc += ucAcc*cutSize\n",
    "        totalDataRate += dataRate*cutSize\n",
    "        start += batchSize\n",
    "        end += batchSize\n",
    "        \n",
    "    outputs = {\n",
    "        \"Accuracy\":totalAcc/dataSize,\n",
    "        \"TotalEvidenceTrue\":testEvTrue/dataSize,\n",
    "        \"TotalEvidenceFalse\":testEvFail/dataSize,\n",
    "        \"UncertaintyAccuracy\":totalUcAcc/dataSize,\n",
    "        \"DataRate\":totalDataRate/dataSize,\n",
    "        \"Truth\":testTruth,\n",
    "        \"Prediction\":testPred,\n",
    "        \"Scores\":testScores,\n",
    "        \"Top3\":top3\n",
    "    }\n",
    "        \n",
    "    return outputs\n",
    "    #return (totalAcc/dataSize,testTruth,testPred,testEvTrue/dataSize,testEvFail/dataSize,totalUcAcc/dataSize,totalDataRate/dataSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(nnModel, iterations, trainData, trainTarget, testData, testTarget, configs, accList):\n",
    "    batcher = DataLoader.DataHandler.batchIterator(trainData, trainTarget, configs[\"batchSize\"])\n",
    "    sample,_ = next(batcher)\n",
    "    \n",
    "    print(\"trainData shape : \", trainData.shape)\n",
    "    print(\"testData shape : \", testData.shape)\n",
    "    print(\"trainTarget shape : \", trainTarget.shape)\n",
    "    print(\"testTarget shape : \", testTarget.shape)\n",
    "    \n",
    "    htTestAcc=0\n",
    "    fold0TestAcc = 0\n",
    "    ucAcc = 0\n",
    "    dataRate = 0\n",
    "    \n",
    "    L_test_ev_s=[]\n",
    "    L_test_ev_f=[]\n",
    "    \n",
    "    print(\"\")\n",
    "    for i in range(iterations):\n",
    "        data, target = next(batcher)\n",
    "        dataClean = data\n",
    "\n",
    "        if(configs[\"PreEmbed\"]):\n",
    "            data = EmbedModel.vectorizeBatch(data)\n",
    "\n",
    "        tokens_length = getTokenLengths(data)\n",
    "        fd = {nnModel.nn_inputs:dataClean, nnModel.nn_vector_inputs:data,nnModel.nn_outputs:target,\n",
    "              nnModel.isTraining:True,nnModel.token_lengths:tokens_length,nnModel.annealing_step:0.00005*i}\n",
    "        _, acc, los = sess.run([nnModel.train_op,nnModel.accuracy,nnModel.loss],feed_dict=fd)\n",
    "\n",
    "        if(i%20==0):\n",
    "            title = (\"[Current iteration = \"+str(i)+\" Train Acc:\"+str(acc)+\" HT Test Acc:\"+str(htTestAcc)+\" fold0Test: (\"+str(fold0TestAcc)+') ucAcc :'+str(ucAcc)\n",
    "                +\" dataRatio  :\"+str(dataRate)+' ]')\n",
    "            title = str(title)       \n",
    "            print(title, end=\"\\r\")\n",
    "\n",
    "        if(i%50000==0 and i != 0):\n",
    "            oldTestAcc = fold0TestAcc               \n",
    "            testOutputs = evaluatePerformance(nnModel, sess, testData, testTarget, configs[\"batchSize\"], 0.1)  \n",
    "            \n",
    "            fold0TestAcc = testOutputs[\"Accuracy\"]\n",
    "            fEvTrue = testOutputs[\"TotalEvidenceTrue\"]\n",
    "            fEvFail = testOutputs[\"TotalEvidenceFalse\"]\n",
    "            ucAcc = testOutputs[\"UncertaintyAccuracy\"]\n",
    "            dataRate = testOutputs[\"DataRate\"]\n",
    "            fTruth = testOutputs[\"Truth\"]\n",
    "            fPrediction = testOutputs[\"Prediction\"]\n",
    "            \n",
    "            confidences = [0.995,0.98,0.90,0.70,0.5]\n",
    "            confidenceMatrix = np.zeros(shape=[len(confidences),3])\n",
    "            for idx in range(len(confidences)):\n",
    "                testOutputs = evaluatePerformance(nnModel, sess, testData, testTarget, configs[\"batchSize\"],1-confidences[idx])\n",
    "                confidenceMatrix[idx,0] = confidences[idx]\n",
    "                confidenceMatrix[idx,1] = testOutputs[\"DataRate\"]\n",
    "                confidenceMatrix[idx,2] = testOutputs[\"UncertaintyAccuracy\"]\n",
    "            \n",
    "            L_test_ev_s.append(fEvTrue)\n",
    "            L_test_ev_f.append(fEvFail)\n",
    "            \n",
    "            if(fold0TestAcc>oldTestAcc):\n",
    "                pass\n",
    "                #saveSession(sess)\n",
    "\n",
    "            accList.append([i, acc, htTestAcc, fold0TestAcc, los, ucAcc])\n",
    "            npAccList = np.array(accList)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputSize = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fullvectorsize:  300\n",
      "(?, 126, 1, 250)\n",
      "WARNING:tensorflow:From C:\\Users\\acer\\Jupyter Notebook\\MedSpecSearch-LRP-and-UserQuery-master\\Models.py:180: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "INFO:tensorflow:Restoring parameters from NNModels/icliniq14k_GoogleNews_onelayer_pad128/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "should_load = True\n",
    "model_path = \"NNModels/icliniq14k_GoogleNews_onelayer_pad128/model.ckpt\"\n",
    "\n",
    "configs[\"maxLength\"] = 128 \n",
    "inputSize = configs[\"maxLength\"]\n",
    "configs[\"batchSize\"] = 8\n",
    "# ORIGINAL PART\n",
    "nnModel = Models.PyramidCNNVShort(inputSize=inputSize, vectorSize=vectorSize, outputSize=outputSize)\n",
    "\n",
    "# MY PART\n",
    "# nnModel = Models.myModel_CNN_TEXT(inputSize=inputSize, vectorSize=vectorSize, outputSize=outputSize)\n",
    "\n",
    "sess = tf.InteractiveSession(graph=nnModel.paperGraph)\n",
    "tf.global_variables_initializer().run()\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "if should_load:\n",
    "    tf.train.Saver().restore(sess, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "confidence = 0.9\n",
    "results = evaluatePerformance(nnModel, sess, testData, testTarget, 1, 1-confidence)\n",
    "results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_difference(list1, list2, truth):\n",
    "    same = 0\n",
    "    diff = 0\n",
    "    correct_to_wrong = 0\n",
    "    wrong_to_correct = 0\n",
    "    for i in range(len(list1)):\n",
    "        if list1[i] == list2[i]:\n",
    "            same += 1\n",
    "        else:\n",
    "            diff += 1\n",
    "            \n",
    "            if list1[i] == truth[i]:\n",
    "                correct_to_wrong += 1\n",
    "            elif list2[i] == truth[i]:        \n",
    "                wrong_to_correct += 1\n",
    "\n",
    "    print(\"same\", same)\n",
    "    print(\"different\", diff)\n",
    "    print(\"correct_to_wrong\", correct_to_wrong)\n",
    "    print(\"wrong_to_correct\", wrong_to_correct)\n",
    "    print(\"wrong_to_wrong\", diff - correct_to_wrong - wrong_to_correct)\n",
    "    \n",
    "    return same, diff, correct_to_wrong, wrong_to_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = results[\"Truth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same 77\n",
      "different 11\n",
      "correct_to_wrong 3\n",
      "wrong_to_correct 4\n",
      "wrong_to_wrong 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77, 11, 3, 4)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_difference(pred_sum, pred_tr_sum, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same 72\n",
      "different 16\n",
      "correct_to_wrong 7\n",
      "wrong_to_correct 7\n",
      "wrong_to_wrong 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(72, 16, 7, 7)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_difference(pred_desc, pred_tr_desc, truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understandin NN - LRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get layers from output to input so that we can backpropagate.\n",
    "\n",
    "Then we calculate word importances for each word in input.\n",
    "\n",
    "In the current model there is only one conv-pool layer so the layer_count is 1. But in the medspecsearch models have 3 layers, so this model is different. We will use this model for LRP purposes.\n",
    "\n",
    "( Maybe remove stop words? )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights, biases and activations to use in lrp method\n",
    "weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='.*kernel.*')\n",
    "biases = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='.*bias.*')\n",
    "\n",
    "activations = []\n",
    "if layer_count == 1:\n",
    "    activations = [nnModel.cnnInput, nnModel.conv1, nnModel.blockPool, nnModel.h_pool_flat, nnModel.fc1, nnModel.scores]\n",
    "    \n",
    "elif layer_count == 3:   \n",
    "    activations = [nnModel.cnnInput, nnModel.conv1, nnModel.blockPool, nnModel.conv2, nnModel.blockPool2, nnModel.conv3,\n",
    "             nnModel.blockPool3, nnModel.h_pool_flat, nnModel.fc1, nnModel.scores]\n",
    "\n",
    "weights.reverse()\n",
    "biases.reverse()\n",
    "activations.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have three parallel conv-pool couple.\n",
    "# We need to split this ben backpropogating\n",
    "# I was experiencing lots of bugs so I splitted it like this\n",
    "# Need a better way for this\n",
    "if layer_count == 3:\n",
    "    biases_0 = np.array(biases)[[0,1,4]]\n",
    "    weights_0 = np.array(weights)[[0,1,4]]\n",
    "    activations_0 = np.array(activations)[[0,1,2,7,8,9]]\n",
    "\n",
    "    biases_1 = np.array(biases)[[0,1,3]]\n",
    "    weights_1 = np.array(weights)[[0,1,3]]\n",
    "    activations_1 = np.array(activations)[[0,1,2,5,6,9]]\n",
    "\n",
    "    biases_2 = np.array(biases)[[0,1,2]]\n",
    "    weights_2 = np.array(weights)[[0,1,2]]\n",
    "    activations_2 = np.array(activations)[[0,1,2,3,4,9]]\n",
    "\n",
    "    biases_splitted = [biases_0, biases_1, biases_2]\n",
    "    weights_splitted = [weights_0, weights_1, weights_2]\n",
    "    activations_splitted = [activations_0, activations_1, activations_2]\n",
    "    pool_biases = [[1,126,1,1], [1,125,1,1], [1,124,1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test\n",
    "batch_x = trainData[0:21]\n",
    "batch_y = trainTarget[0:21]\n",
    "batch_x = EmbedModel.vectorizeBatch(batch_x)\n",
    "batch_y = sess.run(tf.one_hot(batch_y,outputSize)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "backprop_layers = lrp.lrp_layers(alpha, layer_count, activations, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_importances, results_combined = lrp.get_word_relevances(alpha, backprop_layers, layer_count, batch_x[0:1], trainData[0], sess, nnModel, activations, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', -0.008268319347522032),\n",
       " ('doctor', -0.10804220545688706),\n",
       " ('i', 0.020342834952000215),\n",
       " ('have', -0.3017208265593638),\n",
       " ('burning', 0.44691333951505163),\n",
       " ('sensation', -0.005695214677773492),\n",
       " ('while', -0.006010371839961347),\n",
       " ('urinating', -0.016021748390750764),\n",
       " ('and', 0.0),\n",
       " ('a', 0.0),\n",
       " ('frequent', 0.16106670266116513),\n",
       " ('urge', -0.012891035388308568),\n",
       " ('to', 0.0),\n",
       " ('urinate', -0.7028770820335325),\n",
       " ('can', 0.04179418524252254),\n",
       " ('it', 0.09591374824574718),\n",
       " ('be', 0.2434632438935445),\n",
       " ('due', -0.12945842708610766),\n",
       " ('to', 0.0),\n",
       " ('sexual', -0.08989809784105451),\n",
       " ('contact', -0.2738250168551916),\n",
       " ('i', 0.020263399167046985),\n",
       " ('am', -0.007761515735184679),\n",
       " ('a', 0.0),\n",
       " ('year', 0.0011631088119264858),\n",
       " ('old', -0.002362008129093596),\n",
       " ('male', -0.07802539621714201),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0)]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_imps_all_classes(path, default=True):\n",
    "    dir_ = \"default//\"\n",
    "    if not default:\n",
    "        dir_ = \"stemmed//\"\n",
    "        \n",
    "    files = os.listdir(path + dir_ + \"//\")\n",
    "    word_imps_all_classes = []\n",
    "    for file in files:\n",
    "        f = open(path + dir_ + \"//\" + file)\n",
    "        tmp = []\n",
    "        for line in f:\n",
    "            tmp.append(line[0:-1].split(' '))\n",
    "        tmp = tmp[1:] # remove title\n",
    "        word_imps_all_classes.append(tmp)\n",
    "    \n",
    "    return word_imps_all_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for asking keywords to user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_input(user_input):\n",
    "    user_input = DataLoader.DataHandler.cleanTextData(user_input)\n",
    "    user_input = np.array(DataLoader.DataHandler.textIntoWordList(user_input, 128)[0])\n",
    "    \n",
    "    return user_input\n",
    "\n",
    "def get_relevant_words(confidence_top3, amount, tfidf_words):\n",
    "    relevant_words = []\n",
    "    for i in range(len(confidence_top3)):\n",
    "        category = confidence_top3[i][0]\n",
    "        \n",
    "        for words in tfidf_words[ClassDict[category]][0:amount]:      \n",
    "            relevant_words.append(words)\n",
    "    \n",
    "    return relevant_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User input Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first option when asking keywords to user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symptom_input_similarity(index, all_results, raw_user_input):\n",
    "    user_result = all_results[\"Scores\"][0]\n",
    "    \n",
    "    # calculate similarities for with symptoms\n",
    "    cosine_similarities = []\n",
    "    for i in range(len(softmax_results_symcat)):\n",
    "        cosine_similarities.append(cosine_similarity(user_result, softmax_results_symcat[i]))\n",
    "\n",
    "    cosine_similarities = np.array([a[0][0] for a in cosine_similarities])\n",
    "    \n",
    "    # get most similar symptom indexes\n",
    "    highest_indexes = cosine_similarities.argsort()[:0:-1] \n",
    "    \n",
    "    # we dont want to ask user symptoms that user already asked, so remove them\n",
    "    # if keyword is present in user data skip it\n",
    "    highest_relation = list(symptoms[highest_indexes])\n",
    "    for item in raw_user_input.split(\" \"):\n",
    "        for i, keyword in enumerate(highest_relation):\n",
    "            if item in keyword:\n",
    "                del highest_relation[i]\n",
    "\n",
    "    return highest_relation[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_user_input = \"my hair is transparent\" # example input\n",
    "user_input = process_user_input([raw_user_input])\n",
    "all_results = evaluatePerformance(nnModel, sess, user_input, [0], 1, 1-confidence)\n",
    "\n",
    "user_result = all_results[\"Scores\"][0] # we will check similarity between user_result and softmax_results_symcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blood in urine'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example \n",
    "# get most relevant symptom and ask this to user\n",
    "get_symptom_input_similarity(0, all_results, raw_user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vulvar symptoms'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get second most relevant symptom and ask this to user\n",
    "get_symptom_input_similarity(1, all_results, raw_user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity for predicted top3 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second option when asking keywords to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(text_array):\n",
    "    result = \"\"\n",
    "    \n",
    "    for item in text_array:\n",
    "        result += item + \" \"\n",
    "        \n",
    "    result = result[0:-1]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_symptom_top3class(index, choice,all_results, raw_user_input):\n",
    "    \"\"\"choice selects which class we should get keywords from, first second or third, first having highest confidence \"\"\"\n",
    "    # top3 classes for this prediction\n",
    "    user_top3 = np.array(all_results[\"Top3\"][0])[:,0]\n",
    "    user_result = all_results[\"Scores\"][0] \n",
    "    \n",
    "    symptom_index = cosine_sim_indexes_all_classes[ClassDict[user_top3[choice]]][index]\n",
    "    keyword_to_ask = symptoms[symptom_index]\n",
    "    \n",
    "    symptom_words = keyword_to_ask.split(' ')\n",
    "    symptom_word_count = len(symptom_words)\n",
    "    \n",
    "    # count how many words user already explained\n",
    "    included_word_count = 0\n",
    "    for word in raw_user_input.split(' '):\n",
    "        if word in symptom_words:\n",
    "            included_word_count +=1\n",
    "    \n",
    "    # if explained words are more than 66% of symptom words then skip to next symptom\n",
    "    if included_word_count / symptom_word_count > 0.66:\n",
    "        return get_next_symptom(index+1, choice, user_results, raw_user_input)\n",
    "    \n",
    "    return keyword_to_ask   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains similarities for all classes with all symptoms\n",
    "# for example, cosine_sim_indexes_all_classes[0] is Dermatology (ClassDict)\n",
    "# cosine_sim_indexes_all_classes[0][0] is the index most relevant symptom in symptoms array\n",
    "cosine_sim_indexes_all_classes = np.load(\"data//symcat//cosine_sim_indexes_all_classes.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    " # example input\n",
    "raw_user_input = \"my hair is transparent\"\n",
    "\n",
    "user_input = process_user_input([raw_user_input])\n",
    "all_results = evaluatePerformance(nnModel, sess, user_input, [0], 1, 1-confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'skin irritation'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "# return most relevant keyword for top rated category-class and ask this to user\n",
    "get_next_symptom_top3class(0, 0,all_results, raw_user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mouth symptoms'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return second most relevant keyword for top rated category-class and ask this to user\n",
    "get_next_symptom_top3class(1, 0, all_results, raw_user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'problems with lymph nodes (glands)'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return most relevant keyword for second top rated category-class and ask this to user\n",
    "get_next_symptom_top3class(0, 1, all_results, raw_user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
